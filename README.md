# TensorFlow in Action â€” Chapter Summary

This README provides a concise overview of every chapter in *TensorFlow in Action (Thushan Ganegedara)*, outlining the main ideas covered throughout the book.

---

## ðŸ“˜ Part 1 â€” Foundations of TensorFlow 2 and Deep Learning

### *Chapter 1 â€” The Amazing World of TensorFlow*  
Introduces machine learning concepts, explains what TensorFlow is, compares CPU/GPU/TPU hardware, and clarifies when TensorFlow should or shouldnâ€™t be used.

### *Chapter 2 â€” TensorFlow 2*  
Covers TensorFlowâ€™s core componentsâ€”tensors, variables, operationsâ€”and explains how TensorFlow executes computation graphs in an optimized way.

### *Chapter 3 â€” Keras and Data Retrieval in TensorFlow 2*  
Explains the Keras model-building APIs (Sequential, Functional, Subclassing) and shows how to load and preprocess data using tf.data, data generators, and tensorflow-datasets.

### *Chapter 4 â€” Dipping Toes in Deep Learning*  
Introduces essential deep learning architectures: fully connected networks, CNNs, and RNNs, using simple hands-on examples.

### *Chapter 5 â€” State-of-the-Art in Deep Learning: Transformers*  
Breaks down the Transformer architecture, including self-attention, encoderâ€“decoder structure, and why Transformers outperform older models.

---

## ðŸ“˜ Part 2 â€” Look Ma, No Hands! Deep Networks in the Real World

### *Chapter 6 â€” Image Classification with CNNs*  
Builds an end-to-end image classification pipeline using CNNs, including exploratory data analysis and implementing the Inception model.

### *Chapter 7 â€” Improving CNNs and Making Them Explainable*  
Discusses regularization techniques (dropout, augmentation, early stopping), transfer learning, CNN variations, and Grad-CAM for interpretability.

### *Chapter 8 â€” Image Segmentation*  
Introduces semantic segmentation and implements DeepLabv3 with atrous convolutions, including a fully optimized tf.data pipeline.

### *Chapter 9 â€” NLP with TensorFlow: Sentiment Analysis*  
Walks through text preprocessing, tokenization, embeddings, and building LSTM-based sentiment classification models.

### *Chapter 10 â€” NLP with TensorFlow: Language Modeling*  
Covers language modeling fundamentals, GRU networks, text generation, and decoding strategies such as greedy decoding and beam search.

---

## ðŸ“˜ Part 3 â€” Advanced Deep Networks for Complex Problems

### *Chapter 11 â€” Sequence-to-Sequence Learning (Part 1)*  
Builds a machine translation model using encoderâ€“decoder seq2seq architecture, vectorization layers, and end-to-end training/inference flow.

### *Chapter 12 â€” Sequence-to-Sequence Learning (Part 2)*  
Enhances seq2seq models with attention mechanisms (Bahdanau attention) and visualizes how attention improves translation.

### *Chapter 13 â€” Transformers in Action*  
Applies pretrained Transformer models (BERT, DistilBERT) to NLP tasks, including spam classification and question answering with Hugging Face.

### *Chapter 14 â€” TensorBoard: Monitoring and Visualization*  
Teaches how to track training, visualize metrics, profile performance bottlenecks, and explore embedding spaces using TensorBoard.

### *Chapter 15 â€” TFX: MLOps and Model Deployment*  
Builds a production-ready ML pipeline using TFX, covering data ingestion, schema generation, model training, evaluation, serving with Docker, and API deployment.

---
